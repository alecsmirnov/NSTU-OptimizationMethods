# Лабораторные работы по дисциплине "Методы оптимизации" на факультете ФПМИ, НГТУ


### Папки:
### 1. Методы одномерного поиска
> Реализовать **методы дихотомии**, **золотого сечения**, исследовать их сходимость и провести сравнение по числу вычислений функции для 
достижения заданной точности **eps** от **10^(-1)** до **10^(-7)**.  
> Построить график зависимости количества вычислений минимизируемой функции от десятичного логарифма задаваемой точности.  
> Реализовать **алгоритм поиска интервала, содержащего минимум функции**. Реализовать **метод Фибоначчи**, сравнить его с **методами дихотомии** и **золотого сечения**.  
> Задачу выполнять для функции: **f(x) = (x - 2)^2, x ϵ [-2, 20]**.

### 2. Методы спуска (0-го, 1-го и 2-го порядка и переменной метрики)
> Реализовать два метода поиска экстремума функции: **метод Розенброка**, **метод Бройдена**. Включить в реализуемый алгоритм собственную процедуру, реализующую одномерный поиск по направлению.  
> С использованием разработанного программного обеспечения исследовать алгоритмы на квадратичной функции <img width="200" src="2-quadratic-function.png">, функции Розенброка <img width="200" src="2-rosenbrock-function.png"> и на заданной в соответствии с вариантом тестовой функции, осуществляя спуск из различных исходных точек (не менее двух). Исследовать сходимость алгоритма, фиксируя точность определения минимума/максимума, количество итераций метода и количество вычислений функции в зависимости от задаваемой точности поиска. Результатом выполнения данного пункта должны быть выводы об объёме вычислений в зависимости от задаваемой точности и начального приближения.  
>
> Построить траекторию спуска различных алгоритмов из одной и той же исходной точки с одинаковой точностью. В отчете наложить эту траекторию на рисунок с линиями равного уровня заданной функции. 
>
> Результат работы должен содержать:
> * таблицы с результатами проведенных исследований, где должны быть отражены начальное приближение **x0**, задаваемая точность по функции и переменным **(eps от 10^(-3) до 10^(-7))**, количество итераций, число вычислений целевой функции, найденная точка и значение функции в ней.
> * для каждой целевой функции при точности поиска по переменным и функции **eps = 0.001** и одной начальной точке составить следующую таблицу
>
> |  i  | (xi, yi)  | f(xi, yi) | (S1, S2) | lambda | xi - xi-1 | yi - yi-1 | fi - fi-1 | grad(f) | A |
> |:---:|:---------:| :--------:|:--------:|:------:|:---------:|:---------:|:---------:|:-------:|:-:|
> |  1  |           |           |          |        |           |           |           |         |   |
> |  2  |           |           |          |        |           |           |           |         |   |
> | ... |           |           |          |        |           |           |           |         |   |

### 3. Методы штрафных функций
> Применяя методы поиска минимума 0-го порядка, реализовать программу для решения задачи нелинейного программирования с использованием **метода штрафных функций**.  
Исследовать сходимость **метода штрафных функций** в зависимости от:  
> * выбора штрафных функций,  
> * начальной величины коэффициента штрафа,  
> * стратегии изменения коэффициента штрафа,  
> * начальной точки,  
> * задаваемой точности **eps**.  
>
> Применяя методы поиска минимума 0-го порядка, реализовать программу для решения задачи нелинейного программирования с ограничением типа неравенства (**только пункт а**) с использованием **метода барьерных функций**.  
> Исследовать сходимость **метода барьерных функций** (**только пункт а**) в зависимости от:   
> * выбора барьерных функций,
> * начальной величины коэффициента штрафа,
> * стратегии изменения коэффициента штрафа,
> * начального приближения,
> * задаваемой точности **eps**.
>
> Выполнить для функции:  
> **f(x, y) = (x - y)^2 + 10(x + 5)^2 --> min**  
> при ограничении:  
> а) **x + y >= 0**  
> б) **x = 1 - y**

### 4. Статистические методы поиска
> **Простой случайный поиск**  
> Пусть нам необходимо решить задачу минимизации функции **f(x)** на заданной области **D**.  
В заданной области по равномерному закону выбираем случайную точку **x1** и вычисляем в ней значение функции **y1 = f(x1)**. Затем выбираем таким же образом случайную точку **x2** и вычисляем **y2 = (x2)**. Запоминаем минимальное из этих значений и точку, в которой значение функции минимально. Далее генерируем новую точку. Делаем **N** экспериментов, после чего лучшую точку берем в качестве решения задачи (точку, в которой функция имеет минимальное значение среди всех “случайно” сгенерированных).  
> Оценим число экспериментов, необходимое для определения решения (точки минимума) с заданной точностью в n-мерном прямоугольнике, **x ϵ [A, B]**. Пусть **n** - размерность вектора переменных. Объем n-мерного прямоугольника, в котором ведется поиск минимума, **V = П(Bi - Ai)**.    
> Если необходимо найти решение с точностью **eps_i**, **i = 1..n**, по каждой из переменных, то мы должны попасть в окрестность оптимальной точки с объемом **V_eps = П(eps_i)**.  
> Вероятность попадания в эту окрестность при одном испытании равна **P_eps = V_eps / V**. Вероятность непопадания равна **1 - P_eps**. Испытания независимы, поэтому вероятность непопадания за **N** экспериментов равна **(1 - P_eps)^N**.  
> Вероятность того, что мы найдем решение за **N** испытаний: **P = 1 - (1 - P)^N**.  
Отсюда нетрудно получить оценку необходимого числа испытаний N для определения минимума с требуемой точностью: **N >= ln(1 - P) / ln(1 - P_eps)**.  
Данный алгоритм позволяет найти глобальный экстремум в заданной области. Рассмотрим другие подходы к поиску глобального экстремума.
>
> **Алгоритмы глобального поиска**  
> **Алгоритм 1**. В допустимой области **D** случайным образом выбирают точку **x1 ϵ D**. Приняв эту точку за исходную и используя некоторый детерминированный метод или алгоритм направленного случайного поиска, осуществляется спуск в точку локального минимума **x1\* ϵ D**, в области притяжения которого оказалась точка **x1**.  
Затем выбирается новая случайная точка **x2 ϵ D** и по той же схеме осуществляется спуск в точку локального минимума **x2\* ϵ D**, и т.д.  
Поиск прекращается, как только некоторое заданное число **m** раз не удается найти точку локального экстремума со значением функции, меньшим предыдущих.
>
> **Алгоритм 2**.  
>
> **Алгоритм 3**.
